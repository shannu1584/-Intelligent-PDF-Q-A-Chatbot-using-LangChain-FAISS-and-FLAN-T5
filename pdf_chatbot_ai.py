# -*- coding: utf-8 -*-
"""pdf Chatbot AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FSSPH78fTKXKvbGd-T3Wd2v2Wj6QH2Lw
"""

!pip install flask pyngrok transformers sentence-transformers langchain faiss-cpu PyPDF2 langchain-community

!mkdir -p templates
!mkdir -p static
!mkdir -p uploads

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, render_template, request
# import os
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain.chains.question_answering import load_qa_chain
# from langchain_community.llms import HuggingFacePipeline
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline
# from PyPDF2 import PdfReader
# 
# UPLOAD_FOLDER = "uploads"
# os.makedirs(UPLOAD_FOLDER, exist_ok=True)
# 
# app = Flask(__name__)
# 
# # ------------------- Load models -------------------
# embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
# 
# model_name = "google/flan-t5-base"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
# hf_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_length=512)
# llm = HuggingFacePipeline(pipeline=hf_pipeline)
# 
# # Global memory
# vector_store = None
# pdf_chunks = None
# 
# def process_pdf(file_path):
#     reader = PdfReader(file_path)
#     text = ""
#     for page in reader.pages:
#         page_text = page.extract_text()
#         if page_text:
#             text += page_text + "\n"
#     return text
# 
# @app.route("/", methods=["GET", "POST"])
# def home():
#     global vector_store, pdf_chunks
#     answer = ""
#     uploaded_file = None
# 
#     if request.method == "POST":
#         # Upload PDF
#         if "pdf_file" in request.files:
#             file = request.files["pdf_file"]
#             if file.filename != "":
#                 uploaded_file = os.path.join(UPLOAD_FOLDER, file.filename)
#                 file.save(uploaded_file)
# 
#                 text = process_pdf(uploaded_file)
#                 splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100)
#                 pdf_chunks = splitter.split_text(text)
# 
#                 vector_store = FAISS.from_texts(pdf_chunks, embeddings)
# 
#         # Ask Question
#         if vector_store and "user_question" in request.form:
#             question = request.form["user_question"]
#             docs = vector_store.similarity_search(question, k=5)
#             chain = load_qa_chain(llm, chain_type="refine")
#             answer = chain.run(input_documents=docs, question=question)
# 
#     return render_template("index.html", answer=answer, uploaded_file=uploaded_file)
# 
# if __name__ == "__main__":
#     app.run(host="0.0.0.0", port=8000, debug=False)
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile templates/index.html
# <!DOCTYPE html>
# <html>
# <head>
#     <title>üìÑ PDF Q&A Chatbot</title>
#     <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
# </head>
# <body>
# <div class="container">
#     <h1>üìÑ PDF Q&A Chatbot</h1>
# 
#     <div class="card">
#         <h2>Upload PDF Document</h2>
#         <form method="post" enctype="multipart/form-data">
#             <input type="file" name="pdf_file" accept=".pdf" required>
#             <button type="submit">Upload & Process üöÄ</button>
#         </form>
#         {% if uploaded_file %}
#         <p>‚úÖ Uploaded: {{ uploaded_file.split('/')[-1] }}</p>
#         {% endif %}
#     </div>
# 
#     <div class="card">
#         <h2>Ask a Question</h2>
#         <form method="post">
#             <input type="text" name="user_question" placeholder="Enter your question..." required>
#             <button type="submit">Ask ü§ñ</button>
#         </form>
#         {% if answer %}
#         <div class="result">
#             <h2>Answer:</h2>
#             <p>{{ answer }}</p>
#         </div>
#         {% endif %}
#     </div>
# </div>
# </body>
# </html>
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile static/style.css
# body {
#     font-family: 'Segoe UI', sans-serif;
#     background: linear-gradient(135deg, #141E30, #243B55);
#     color: white;
#     display: flex;
#     justify-content: center;
#     align-items: center;
#     height: 100vh;
#     margin: 0;
# }
# .container {
#     text-align: center;
#     width: 60%;
# }
# h1, h2 {
#     margin-bottom: 15px;
# }
# .card {
#     background: rgba(255, 255, 255, 0.1);
#     padding: 20px;
#     border-radius: 12px;
#     box-shadow: 0 0 15px rgba(0,0,0,0.4);
#     margin-bottom: 20px;
# }
# input, button {
#     width: 90%;
#     margin: 10px 0;
#     padding: 10px;
#     border-radius: 8px;
#     border: none;
# }
# button {
#     background: #FFD700;
#     cursor: pointer;
#     font-weight: bold;
# }
# button:hover {
#     background: #FFA500;
# }
# .result {
#     margin-top: 20px;
#     padding: 15px;
#     background: rgba(255,255,255,0.1);
#     border-radius: 10px;
# }
#

# üî¥ Kill old Flask/ngrok processes
!pkill -f flask || echo "No flask running"
!pkill -f ngrok || echo "No ngrok running"

# üîé List processes using port 8000
!lsof -i :8000

!kill -9 1288

# Run Flask in background
!nohup python app.py > flask.log 2>&1 &

# Start ngrok tunnel
from pyngrok import ngrok, conf
conf.get_default().auth_token = "33pyU8XWfrN8JFX8kwzY8Uf1j7s_612rotnaQX2f58dq3n8Ez"
public_url = ngrok.connect(8000)
print("üåç Public URL:", public_url)

# Optional: show Flask logs
!sleep 3 && tail -n 20 flask.log

!tail -f flask.log



